---
title: "Clustering: K-means"
output: htmldocument
date: '2022-06-14'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#K Means

What is unsupervised learning?

We have $p$ variables for $n$ observations $x1,\dots,xn$, and for observation $i$:

$$x{i1},x{i2},\ldots,x{ip} \sim P$$

- $P$ is a $p$-dimensional distribution that we might not know much about *a priori*.

- unsupervised: none of the variables are response variables, i.e., there are no labeled data

Think of unsupervised learning as an extension of EDA...

- Statistical learning is the process of ascertaining (discovering) associations between groups of variables

- unsupervised learning - where the goal is to discover interesting things about the data


## What is clustering (aka cluster analysis)?

Goals:

- observations within clusters are more similar to each other,

- observations in different clusters are more different from each other

How do we define distance / dissimilarity between observations? 

- e.g. Euclidean distance between observations $i$ and $j$

$$d(xi, xj) = \sqrt{(x{i1}-x{j1})^2 + \cdots + (x{ip}-x{jp})^2}$$

Units matter!

- one variable may dominate others when computing Euclidean distance because its range is much larger

- can standardize each variable / column of dataset to have mean 0 and standard divation 1 with `scale()`

- but we may value the separation in that variable! (so just be careful...)

It is the partitioning of data into homogeneous subgroups

Goal define clusters for which the within-cluster variation is relatively small, i.e. observations within clusters are similar to each other


## What's the clustering objective?

- $C1, \dots, CK$ are sets containing indices of observations in each of the $K$ clusters

  - if observation $i$ is in cluster $k$, then $i \in Ck$

- We want to minimize the within-cluster variation $W(Ck)$ for each cluster $Ck$ and solve:

$$\underset{C1, \dots, CK}{\text{minimize}} \Big\{ \sum{k=1}^K W(Ck) \Big\}$$

- Can define using the squared Euclidean distance ( $|Ck| = nk =$ # observations in cluster $k$)

$$W(Ck) = \frac{1}{|Ck|}\sum{i,j \in Ck} d(xi, xj)^2$$

  - Commonly referred to as the within-cluster sum of squares (WSS)

So how can we solve this?

## [Lloyd's algorithm](https://en.wikipedia.org/wiki/K-meansclustering)

1) Choose $K$ random centers, aka centroids

2) Assign each observation closest center (using Euclidean distance)

3) Repeat until cluster assignment stop changing:

  - Compute new centroids as the averages of the updated groups
  
  - Reassign each observations to closest center

Converges to a local optimum, not the global 

Results will change from run to run (set the seed!)

Takes $K$ as an input!

## Gapminder data

Health and income outcomes for 184 countries from 1960 to 2016 from the famous [Gapminder project](https://www.gapminder.org/data)

```{r load-data, warning = FALSE, message = FALSE}
library(tidyverse)
library(dslabs)
gapminder <- as_tibble(gapminder)
head(gapminder)
```

## GDP is severely skewed right...

```{r gdp-hist, warning = FALSE, message = FALSE, fig.align='center', fig.height=5}
gapminder %>% 
  ggplot(aes(x = gdp)) + 
  geom_histogram(col = "black", fill =  "steelblue") 
```

## Some initial cleaning...

- Each row is at the `country`-`year` level

- Will just focus on data for 2011 where `gdp` is not missing

- Take `log()` transformation of `gdp`

```{r init-tot-rows}
clean_gapminder <- gapminder %>%
  filter(year == 2011, !is.na(gdp)) %>%
  mutate(log_gdp = log(gdp))
```

### K-means clustering example (`gdp` and `life_expectancy`)

- Use the `kmeans()` function, but must provide number of clusters $K$

```{r first-kmeans}
init_kmeans <- 
  kmeans(dplyr::select(clean_gapminder,
                       log_gdp, life_expectancy),
         algorithm = "Lloyd", centers = 4,
         nstart = 1)

clean_gapminder %>%
  mutate(country_clusters = 
           as.factor(init_kmeans$cluster)) %>% #<<
  ggplot(aes(x = log_gdp, y = life_expectancy,
             color = country_clusters)) +
  geom_point() + 
  ggthemes::scale_color_colorblind() +
  theme_bw() +
  theme(legend.position = "right") 
```

## Careful with units...

- Use the `coord_fixed()` so that the axes match with unit scales

```{r coord-fixed}
clean_gapminder %>%
  mutate(country_clusters = 
           as.factor(init_kmeans$cluster)) %>% #<<
  ggplot(aes(x = log_gdp, y = life_expectancy,
             color = country_clusters)) +
  geom_point() + 
  ggthemes::scale_color_colorblind() +
  theme_bw() +
  theme(legend.position = "right") +
  coord_fixed() #<<
```

## Standardize the variables!

- Use the `scale()` function to first standardize the variables, $\frac{value - mean}{standard\ deviation}$

```{r std-kmeans}
clean_gapminder <- clean_gapminder %>%
  mutate(std_log_gdp = as.numeric(scale(log_gdp, center = TRUE, scale = TRUE)), #<<
         std_life_exp = as.numeric(scale(life_expectancy, center = TRUE, scale = TRUE))) #<<

std_kmeans <- 
  kmeans(dplyr::select(clean_gapminder, std_log_gdp, std_life_exp),
         algorithm = "Lloyd", centers = 4, nstart = 1)

clean_gapminder %>%
  mutate(country_clusters = 
           as.factor(std_kmeans$cluster)) %>% #<<
  ggplot(aes(x = log_gdp, y = life_expectancy,
             color = country_clusters)) +
  geom_point() + 
  ggthemes::scale_color_colorblind() +
  theme_bw() +
  theme(legend.position = "right") +
  coord_fixed()
```

## Standardize the variables!

```{r std-kmeans-view}
clean_gapminder %>%
  mutate(country_clusters = 
           as.factor(std_kmeans$cluster)) %>% #<<
  ggplot(aes(x = std_log_gdp, y = std_life_exp,
             color = country_clusters)) +
  geom_point() + 
  ggthemes::scale_color_colorblind() +
  theme_bw() +
  theme(legend.position = "bottom") +
  coord_fixed() #<<
```

### And if we run it again?

We get different clustering results!

```{r second-kmeans}
anotherkmeans <- 
  kmeans(dplyr::select(clean_gapminder, std_log_gdp, std_life_exp),
         algorithm = "Lloyd", centers = 4, nstart = 1)

clean_gapminder %>%
  mutate(country_clusters = 
           as.factor(anotherkmeans$cluster)) %>% #<<
  ggplot(aes(x = log_gdp, y = life_expectancy,
             color = country_clusters)) +
  geom_point() + 
  ggthemes::scale_color_colorblind() +
  theme_bw() +
  theme(legend.position = "right")
```

Results depend on initialization

Keep in mind: the labels / colors are arbitrary

### Fix randomness issue with `nstart`

Run the algorithm `nstart` times, then pick the results with lowest total within-cluster variation (total WSS $= \sum^K W(Ck)$)

```{r nstart-kmeans}
nstartkmeans <- 
  kmeans(dplyr::select(clean_gapminder,
                       std_log_gdp, std_life_exp),
         algorithm = "Lloyd", centers = 4,
         nstart = 30) #<<

clean_gapminder %>%
  mutate(country_clusters = 
           as.factor(nstartkmeans$cluster)) %>% 
  ggplot(aes(x = log_gdp, y = life_expectancy,
             color = country_clusters)) +
  geom_point() + 
  ggthemes::scale_color_colorblind() +
  theme_bw() +
  theme(legend.position = "right")
```

### By default `R` uses [Hartigan and Wong algorithm](https://en.wikipedia.org/wiki/K-meansclustering#Hartigan%E2%80%93Wongmethod)

Updates based on changing a single observation

Computational advantages over re-computing distances for every observation

```{r default-kmeans}
defaultkmeans <- 
  kmeans(dplyr::select(clean_gapminder,
                       std_log_gdp, std_life_exp),
         algorithm = "Hartigan-Wong", #<<
         centers = 4, nstart = 30) 

clean_gapminder %>%
  mutate(country_clusters = 
           as.factor(defaultkmeans$cluster)) %>% 
  ggplot(aes(x = log_gdp, y = life_expectancy,
             color = country_clusters)) +
  geom_point() + 
  ggthemes::scale_color_colorblind() +
  theme_bw() +
  theme(legend.position = "right")
```

Very little differences for our purposes...

### Better alternative to `nstart`: K-means++

Pick a random observation to be the center $c1$ of the first cluster $C1$

  - This initializes a set $Centers = \{c1 \}$
  
Then for each remaining cluster $c^* \in 2, \dots, K$:

  - For each observation (that is not a center), compute $D(xi) = \underset{c \in Centers}{\text{min}} d(xi, c)$
  
    - Distance between observation and its closest center $c \in Centers$

  - Randomly pick a point $xi$ with probability: $pi = \frac{D^2(xi)}{\sum{j=1}^n D^2(xj)}$

  - As distance to closest center increases $\Rightarrow$ probability of selection increases

  - Call this randomly selected observation $c^*$, update $Centers = Centers\ \cup c^*$
  
    - Same as `centers = c(centers, cnew)`
    
Then run $K$-means using these $Centers$ as the starting points

### K-means++ in R using [`flexclust`](https://cran.r-project.org/web/packages/flexclust/flexclust.pdf)

```{r kmeanspp}
library(flexclust)

init_kmeanspp <- 
  kcca(dplyr::select(clean_gapminder, #<<
                     std_log_gdp, std_life_exp), k = 4, #<<
       control = list(initcent = "kmeanspp")) #<<

clean_gapminder %>%
  mutate(country_clusters = 
           as.factor(init_kmeanspp@cluster)) %>% #<< #@ symbol bc init_kmeanspp is an s4 object
  ggplot(aes(x = log_gdp, y = life_expectancy,
             color = country_clusters)) +
  geom_point() + 
  ggthemes::scale_color_colorblind() +
  theme_bw() +
  theme(legend.position = "right")
```

Note the use of `@` instead of `$`...

### So, how do we choose the number of clusters?!

There is no universally accepted way to conclude that a particular choice of $K$ is optimal!

### Popular heuristic: elbow plot (use with caution)

Look at the total within-cluster variation as a function of the number of clusters

```{r kmeans-elbow}
# Initialize number of clusters to search over
n_clusters_search <- 2:12

tibble(totalwss = 
         # Compute total WSS for each number by looping with sapply
         sapply(n_clusters_search,
                function(k) {
                  kmeans_results <- kmeans(dplyr::select(clean_gapminder,
                                                         std_log_gdp,
                                                         std_life_exp),
                                           centers = k, nstart = 30)
                  # Return the total WSS for choice of k
                  return(kmeans_results$tot.withinss)
                })) %>%
  mutate(k = n_clusters_search) %>%
  ggplot(aes(x = k, y = totalwss)) +
  geom_line() + geom_point() +
  scale_x_continuous(breaks = seq(0, 12, by = 2)) +
  labs(x = "Number of clusters K", y = "Total WSS") +
  theme_bw()

```

### Popular heuristic: elbow plot (use with caution)

Choose $K$ where marginal improvements is low at the bend (hence the elbow)

This is just a guideline and should not dictate your choice of $K$!

[Gap statistic](https://web.stanford.edu/~hastie/Papers/gap.pdf) is a popular choice (see [`clusGap` function](https://stat.ethz.ch/R-manual/R-devel/library/cluster/html/clusGap.html) in [`cluster` package](https://cran.r-project.org/web/packages/cluster/cluster.pdf))

Next Tuesday: model-based approach to choosing the number of clusters!

