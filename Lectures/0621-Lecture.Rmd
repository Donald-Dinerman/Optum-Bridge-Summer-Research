-
title: "0621-Lecture"
output: html_document
date: '2022-06-21'
-

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Model-based clustering: Gaussian mixture models

## Mixture models

Make soft assignments: Allow for some uncertainty in the clustering results

We assume the distribution $f(x)$ is a mixture of $K$ component distributions:

$$f(x) = \sum_{k=1}^K \pi_k f_k(x)$$

- $\pi_k =$ mixing proportions (or weights), where $\pi_k > 0$, and $\sum_k \pi_k = 1$

This is a data generating process, meaning to generate a new point:

1. pick a distribution / component among our $K$ options, by introducing a new variable: 

  - $z \sim \text{Multinomial} (\pi_1, \pi_2, \dots, \pi_k)$, i.e. categorical variable saying which group the new point is from

2. generate an observation with that distribution / component, i.e. $x | z \sim f_{z}$

So what do we use for each $f_k$?

## Gaussian mixture models (GMMs)

Assume a parametric mixture model, with parameters $\theta_k$ for the $k$th component

$$f(x) = \sum_{k=1}^K \pi_k f_k(x; \theta_k)$$

Assume each component is [Gaussian / Normal](https://en.wikipedia.org/wiki/Normal_distribution) where for 1D case:

$$f_k(x; \theta_k) = N(x; \mu_k, \sigma_k^2) = \frac{1}{\sqrt{2\pi \sigma_k^2}}\text{exp} \Big( -\frac{(x - \mu_k)^2}{2 \sigma_k^2} \Big)$$

We need to estimate each $\pi_1, \dots, \pi_k$, $\mu_1, \dots, \mu_k$, $\sigma_1, \dots, \sigma_k$!

## Let's pretend we only have one component...

If we have $n$ observations from a single Normal distribution, we estimate the distribution parameters using the likelihood function, the probability / density of observing the data given the parameters

$$\mathcal{L}(\mu, \sigma | x_1, \dots, x_n) = f( x_1, \dots, x_n | \mu, \sigma) =  \prod_i^n \frac{1}{\sqrt{2\pi \sigma^2}}\text{exp }-\frac{(x_i - \mu)^2}{2 \sigma^2}$$

We can compute the maximum likelihood estimates (MLEs) for $\mu$ and $\sigma$

You already know these values!

- $\hat{\mu}_{MLE} = \frac{1}{n} \sum_i^n x_i$, sample mean

- $\hat{\sigma}_{MLE} = \sqrt{\frac{1}{n}\sum_i^n (x_i - \mu)^2}$, sample standard deviation (plug in $\hat{\mu}_{MLE}$)

## The problem with more than one component...

- We don't know which component an observation belongs to

- IF WE DID KNOW, then we could compute each component's MLEs as before

- But we don't know because $z$ is a latent variable! So what about its distribution given the data?

$$P(z_i = k | x_i) = \frac{P(x_i | z_i = k) P(z_i = k)}{P(x_i)}$$

$$=\frac{\pi_{k} N\left(\mu_{k}, \sigma_{k}^{2}\right)}{\sum_{k=1}^{K} \pi_{k} N\left(\mu_{k}, \sigma_{k}\right)}$$

- But we do NOT know these parameters!

- This leads to a very useful algorithm in statistics...

```{r init-sim-data, fig.align='center'}
library(tidyverse)

# mixture components
mu_true <- c(5, 13)
sigma_true <- c(1.5, 2)

# determine Z_i
z <- rbinom(500, 1, 0.75)

# sample from mixture model
x <- rnorm(10000, mean = mu_true[z + 1], 
           sd = sigma_true[z + 1])

tibble(xvar = x) %>%
  ggplot(aes(x = xvar)) +
  geom_histogram(color = "black",
                 fill = "darkblue",
                 alpha = 0.3) +
  theme_bw() +
  labs(x = "Simulated variable",
       y = "Count")
```

## Expectation-maximization (EM) algorithm

We alternate between the following:

- pretending to know the probability each observation belongs to each group, to estimate the parameters of the components

- pretending to know the parameters of the components, to estimate the probability each observation belong to each group

Similar to K-means algorithm!

1. Start with initial guesses about $\pi_1, \dots, \pi_k$, $\mu_1, \dots, \mu_k$, $\sigma_1, \dots, \sigma_k$

2. Repeat until nothing changes:

- Expectation step: calculate $\hat{z}_{ik}$ = expected membership of observation $i$ in cluster $k$

- Maximization step: update parameter estimates with weighted MLE using $\hat{z}_{ik}$

## How does this relate back to clustering?

From the EM algorithm:  $\hat{z}_{ik}$ is a soft membership of observation $i$ in cluster $k$

  - you can assign observation $i$ to a cluster with the largest $\hat{z}_{ik}$
  
  - measure cluster assignment uncertainty $= 1 - \text{max}_k \hat{z}_{ik}$

Our parameters determine the type of clusters

In 1D we only have two options:

1. each cluster is assumed to have equal variance (spread): $\sigma_1^2 = \sigma_2^2 = \dots = \sigma_k^2$

2. each cluster is allowed to have a different variance

But that is only 1D... what happens in multiple dimensions?

## Multivariate GMMs

$$f(x) = \sum_{k=1}^K \pi_k f_k(x; \theta_k) \\ \text{where }f_k(x; \theta_k) \sim N(\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)$$

Each component is a multivariate normal distribution:

- $\boldsymbol{\mu}_k$ is a vector of means in $p$ dimensions

- $\boldsymbol{\Sigma}_k$ is the $p \times p$ covariance matrix - describes the joint variability between pairs of variables

$$\sum=\left[\begin{array}{cccc}
\sigma_{1}^{2} & \sigma_{1,2} & \cdots & \sigma_{1, p} \\
\sigma_{2,1} & \sigma_{2}^{2} & \cdots & \sigma_{2, p} \\
\vdots & \vdots & \ddots & \vdots \\
\sigma_{p, 1} & \sigma_{p, 2}^{2} & \cdots & \sigma_{p}^{2}
\end{array}\right]$$

## Covariance constraints

As we increase the number of dimensions, model fitting and estimation becomes increasingly difficult

We can use constraints on multiple aspects of the $k$ covariance matrices:

- volume: size of the clusters, i.e., number of observations, 

- shape: direction of variance, i.e. which variables display more variance

- orientation: aligned with axes (low covariance) versus tilted (due to relationships between variables)

```{r}
library(magick)

image_read('https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5096736/bin/nihms793803f2.jpg') %>% print()
```
- Control volume, shape, orientation

- E means equal and V means variable (VVV is the most flexible, but has the most parameters)

- Two II is spherical, one I is diagonal, and the remaining are general

## Bayesian information criterion (BIC)

This is a statistical model

$$f(x) = \sum_{k=1}^K \pi_k f_k(x; \theta_k) \\ \text{where }f_k(x; \theta_k) \sim N(\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)$$

Meaning we can use a model selection procedure for determining which best characterizes the data

Specifically - we will use a penalized likelihood measure

$$BIC = 2\log \mathcal{L} - m\log n$$

- $\log \mathcal{L}$ is the log-likelihood of the considered model

- with $m$ parameters (VVV has the most parameters) and $n$ observations

- penalizes large models with many clusters without constraints

- we can use BIC to choose the covariance constraints AND number of clusters $K$!

_The above BIC is really the -BIC of what you typically see, this sign flip is just for ease_

### How do we implement this? Back to our NBA data... enter `mclust`

```{r load-data, warning = FALSE, message = FALSE}
library(tidyverse)

nba_pos_stats <- read_csv("http://www.stat.cmu.edu/cmsac/sure/2022/materials/data/sports/clustering/nba_2022_player_per_pos_stats.csv")

tot_players <- nba_pos_stats %>% filter(tm == "TOT")

nba_player_stats <- nba_pos_stats %>% filter(!(player %in% tot_players$player)) %>% 
  bind_rows(tot_players)

nba_filtered_stats <- nba_player_stats %>% filter(mp >= 125)

head(nba_filtered_stats)
```

While there are multiple implementations of GMMs, we will use the [`mclust`](https://cran.r-project.org/web/packages/mclust/vignettes/mclust.html) package

```{r load-mclust, message=TRUE, warning = TRUE}
library(mclust)
```

## Selecting the model and number of clusters

Use the `Mclust` function to search over 1 to 9 clusters (_K_ = `G`) and the different covariance constraints (i.e. models) 

```{r nba-mclust, eval = FALSE}
nba_mclust <- Mclust(dplyr::select(nba_filtered_stats, x3pa, trb))
```

We can use the `summary()` function to display the selection and resulting table of assignments:

```{r nba-mclust-summary}
summary(nba_mclust)
```

## Display the BIC for each model and number of clusters

```{r nba-bic, fig.align = 'center', fig.height=6}
plot(nba_mclust, what = 'BIC', 
     legendArgs = list(x = "bottomright", ncol = 7))
```

```{r nba-cluster-plot, fig.height=6} 
plot(nba_mclust, what = 'classification')
```

## How do the cluster assignments compare to the positions?

We can again compare the clustering assignments with player positions:

```{r nba-table}
table("Clusters" = nba_mclust$classification, "Positions" = nba_filtered_stats$pos)
```

-

## What about the cluster probabilities?

.pull-left[
```{r nba-probs, eval = FALSE}
nba_player_probs <- nba_mclust$z #<<
colnames(nba_player_probs) <- 
  paste0('Cluster ', 1:3)

nba_player_probs <- nba_player_probs %>%
  as_tibble() %>%
  mutate(player = 
           nba_filtered_stats$player) %>%
  pivot_longer(contains("Cluster"), #<<
               names_to = "cluster", #<<
               values_to = "prob") #<<

nba_player_probs %>%
  ggplot(aes(prob)) +
  geom_histogram() +
  theme_bw() +
  facet_wrap(~ cluster, nrow = 2)

```
]
.pull-right[
```{r ref.label='nba-probs', echo=FALSE}

```
]

-

## Which players have the highest uncertainty?

.pull-left[
```{r nba-uncertainty, eval = FALSE}
nba_filtered_stats %>%
  mutate(cluster = #<<
           nba_mclust$classification, #<<
         uncertainty = #<<
           nba_mclust$uncertainty) %>%#<<
  group_by(cluster) %>%
  arrange(desc(uncertainty)) %>%
  slice(1:5) %>%
  ggplot(aes(y = uncertainty, 
             x = reorder(player, #<<
                         uncertainty))) +#<<
  geom_point() +
  coord_flip() + 
  theme_bw() +
  facet_wrap(~ cluster, 
             scales = 'free_y', nrow = 3)

```
















































